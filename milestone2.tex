\documentclass{article}
\usepackage{tocbibind, url}
\usepackage{amsmath,amssymb,amsopn,amsthm}
\title{CRYBABY --- Milestone 2 Report}
\author{Ashwin Bhat, Joshua Cranmer, Matt Greenland}
\begin{document}
\maketitle
\tableofcontents

\section{Progress} % (fold)

\subsection{Webscraping}

In terms of grabbing the data from the web, this section is more or less
complete. While data has yet to be generalized, code now exists to go all the
way from a search query to extraction of parse trees for sentences.

Extraction of text from the web proceeds by removing large swathes of the
webpage's source HTML that could screw up the text dumper (such as
\texttt{<script>} tags). Then the scraper attempts to identify certain common
characteristics indicating main content of the webpage or user comments on the
webpage, and limits its output to that portion of the webpage. If it cannot
find such tags, to be on the conservative side, it uses the entire page to at
least provide some measure of text.

After extracting the text content, the text is split into sentences. The current
method of sentence splitting is simplistic, looking for punctuation followed by
whitespace or for a large amount of whitespace (an artifact of how the text of
the webpage is extracted which proves to be a reasonable heuristic for breaks
suggested by page layout). These sentences are then filtered by removing very
short sentences.

The final step is to produce parse trees of the sentences. This is done using
the Stanford PCFG parser. After producing parse trees for all sentences, any
sentence which lacks a verb is discarded, which does a good job of also removing
extraneous content not handled by the original parser. At this point, the
webpage is now ready to be handed off to other components.

As potential improvements to our code, we could use freely available components
to provide more accurate web parsing code to increase our coverage. We could
also modify the text extractor to output the text in a format that is more
amenable to determination of sentence boundaries.

\subsection{Jargon Identification} % (fold)

In the jargon identification segment, we have made some changes to the way terms are learned.  We are now using a function to indicate similarity between bags of words:\begin{align*}
	similarity(A, B) &= \frac{|A\cap B|}{|A\cup B|}.
\end{align*}This way, the similarity is always a number between 0 and 1.  We are currently using this similarity measure, with a threshold, to find all similarities between noun phrase bags.  This way, terms like ``battery'' and ``battery life'' can be grouped together, making the program more likely to learn that battery life is an important feature to identify.  Non-extensive testing seems to indicate that this approach is working well, though some extra tweaks may be necessary to eliminate some unnecessary words besides stop words that are already being omitted.

One issue that we have noticed is dealing with different versions of the same word (i.e., words with the same stem).  We plan to integrate the Porter Stemmer \cite{porter} into our system to recognize common stems.

% subsection Jargon Identification (end)

\subsection{Cool Stuff} % (fold)



% subsection Cool Stuff (end)

\subsection{Data Gathering} % (fold)



% subsection Data Gathering (end)

% section Progress (end)

\tocsection
\bibliographystyle{acm}
\bibliography{refs}
\end{document}
